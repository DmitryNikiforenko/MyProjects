{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Telegram Bot\n",
        "* здесь три ячейки для самодостаточной работы с ботом.\n",
        "* сначала загружаем всё необходимое окружение, после запускаем последнюю ячейку с ботом и переходим в мессенджер.\n"
      ],
      "metadata": {
        "id": "zx6ePE3J7ks9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL6fLIIqC2sD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa89f40-42d3-4074-8b0a-78ec6453226c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-telegram-bot==13.3\n",
            "  Downloading python_telegram_bot-13.3-py3-none-any.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.3/436.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.3) (2023.11.17)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.3) (6.3.2)\n",
            "Collecting APScheduler==3.6.3 (from python-telegram-bot==13.3)\n",
            "  Downloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.3) (2023.3.post1)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.3) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.3) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.3) (5.2)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop_words, annoy, docopt\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32895 sha256=116a8e57f9b544df0bab1b69e49383da94f700eed3ad91c3a9273bac3a410adf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=550738 sha256=4958fa815757761b7329e9d9da3be4bf35c71392689436b60c9d65503a39bd27\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2b4a7237e6fc472225a5cba4408dd6e14689b7cb87625340c7ff3dfc1af89f29\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built stop_words annoy docopt\n",
            "Installing collected packages: stop_words, pymorphy2-dicts-ru, docopt, dawg-python, annoy, pymorphy2, num2words, APScheduler, python-telegram-bot\n",
            "Successfully installed APScheduler-3.6.3 annoy-1.17.3 dawg-python-0.7.2 docopt-0.6.2 num2words-0.5.13 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 python-telegram-bot-13.3 stop_words-2018.7.23\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.25.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install python-telegram-bot==13.3 pymorphy2 stop_words annoy num2words\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "\n",
        "import os\n",
        "import mmap\n",
        "import re\n",
        "from telegram.ext import Updater, CommandHandler, MessageHandler, filters\n",
        "import string\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from stop_words import get_stop_words\n",
        "import annoy\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "import num2words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/gpt2small_based_tales_model/gpt_chf\")\n",
        "model1 = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/gpt2small_based_tales_model/model_gpt_chf\")\n",
        "\n",
        "morpher = MorphAnalyzer()\n",
        "sw = set(get_stop_words(\"ru\"))\n",
        "\n",
        "def preprocess_txt(line):\n",
        "    spls = ' '.join([re.sub(r'={3}', ' === ', i) for i in line.split()]).split()\n",
        "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
        "    spls = [re.sub(r'[^а-яА-Яё=== ]', '', i) for i in spls]\n",
        "    spls = [i for i in spls if len(i)>2]\n",
        "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
        "    return spls\n",
        "\n",
        "# Основная функция преобразования текста в вектор х100\n",
        "\n",
        "def embed_txt(txt, idfs, midf):\n",
        "    n_ft = 0\n",
        "    vector_ft = np.zeros(100)\n",
        "    for word in txt:\n",
        "        if word in modelFT.wv:\n",
        "            vector_ft += modelFT.wv[word] * idfs.get(word, midf)\n",
        "            n_ft += idfs.get(word, midf)\n",
        "    return vector_ft / n_ft\n",
        "\n",
        "vectorizer = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"/content/drive/MyDrive/chat_bot_telegram/vectorizer.vocabulary.pkl\", \"rb\")))\n",
        "\n",
        "lr = pickle.load(open('/content/drive/MyDrive/chat_bot_telegram/lr_model.sav', 'rb'))\n",
        "\n",
        "with open('/content/drive/MyDrive/chat_bot_telegram/idfs.pkl', 'rb') as f:\n",
        "    idfs = pickle.load(f)\n",
        "\n",
        "#  Загрузим индексы #Tfidf\n",
        "\n",
        "idfs = pd.read_pickle('/content/drive/MyDrive/chat_bot_telegram/idfs.pkl')\n",
        "midf = np.mean([i for i in idfs.values()])\n",
        "\n",
        "ft_index = annoy.AnnoyIndex(100, 'angular')\n",
        "ft_index.load('/content/drive/MyDrive/chat_bot_telegram/speaker.ann')\n",
        "index_map = pd.read_pickle(\"/content/drive/MyDrive/chat_bot_telegram/index_speaker.pkl\")\n",
        "\n",
        "ft_index_shop = annoy.AnnoyIndex(100, 'angular')\n",
        "ft_index_shop.load('/content/drive/MyDrive/chat_bot_telegram/shop.ann')\n",
        "index_map_shop = pd.read_pickle(\"/content/drive/MyDrive/chat_bot_telegram/index_shop.pkl\")\n",
        "\n",
        "ft_index_pharm = annoy.AnnoyIndex(100 ,'angular')\n",
        "ft_index_pharm.load('/content/drive/MyDrive/chat_bot_telegram/pharm.ann')\n",
        "index_map_pharm = pd.read_pickle(\"/content/drive/MyDrive/chat_bot_telegram/index_pharm.pkl\")\n",
        "\n",
        "#загрузка FastText\n",
        "modelFT = FastText.load(\"/content/drive/MyDrive/chat_bot_telegram/ft_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3sxuG-hVJ0S",
        "outputId": "d70422c4-9cd2-4b08-d5f1-13d9ce2cd468",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# заменить на свой токен\n",
        "updater = Updater(\"6315571996:AAGjw8OBxcjqGPozsN0HA-xjW_VIsQljms8\", use_context=True) # Токен API к Telegram\n",
        "dispatcher = updater.dispatcher\n",
        "\n",
        "def startCommand(update, context):\n",
        "    context.bot.send_message(chat_id=update.message.chat_id, text='Вас приветствует NDS_NLP_bot. Я могу поговорить с вами на свободную тему, выполнить подбор 5 наиболее подходящих товаров одежды по запросу, показать 3 лекарственных препарата по указанной вами фармакологической группе и форме выпуска, рассказать вам сказку по предложеному Вами её началу. Чего вы хотите?')\n",
        "\n",
        "def textMessage(update, context):\n",
        "\n",
        "    input_txt = preprocess_txt(update.message.text)\n",
        "    vect = vectorizer.transform([\" \".join(input_txt)])\n",
        "    prediction = lr.predict(vect)\n",
        "\n",
        "\n",
        "    # prod_shop - товары магазина\n",
        "    if prediction[0] == 1:\n",
        "        vect_ft = embed_txt(input_txt, idfs, midf)\n",
        "        ft_index_shop_val = ft_index_shop.get_nns_by_vector(vect_ft, 5)\n",
        "        for item in ft_index_shop_val:\n",
        "            title, image = index_map_shop[item]\n",
        "            context.bot.send_message(chat_id=update.message.chat_id, text=\"title: {} image: {}\".format(title, image))\n",
        "        return\n",
        "\n",
        "    #Tale - сказки от GPT2_small\n",
        "    if prediction[0] == 2:\n",
        "      #модель выбирает один из предложеных префиксов для генерации текста\n",
        "      phrases_list = ['Жили были старик со старухою ', \"Как то раз в один прекрасный день \", \"Однажды произошла интересная история \", \"У моего лучшего друга \", \"Эта история о том, как я повстречал Вашу маму \", \"В этот прекрасный день я встретил свою собаку\", \"Когда-то, давным давно, \", \"Это очень грусная история о том, как\", \"Однажды я решил просто прогуляться и \"]\n",
        "      prefix = phrases_list[np.random.randint(len(phrases_list))]\n",
        "      tokens = tokenizer(prefix, return_tensors='pt')\n",
        "      size = tokens['input_ids'].shape[1]\n",
        "      output = model1.generate(\n",
        "        **tokens,\n",
        "        #end_token=end_token_id,\n",
        "        do_sample=False,\n",
        "        max_length=size+100,\n",
        "        repetition_penalty=5.,\n",
        "        temperature=0.2,\n",
        "        num_beams=15,\n",
        "      )\n",
        "\n",
        "      decoded = tokenizer.decode(output[0])\n",
        "      result = decoded[len(prefix):]\n",
        "      context.bot.send_message(chat_id=update.message.chat_id, text=prefix + result)\n",
        "      return\n",
        "\n",
        "    #pharm - лекарственные препараты по запросу фармакологической группы и формы выпуска\n",
        "    if prediction[0] == 3:\n",
        "      vect_ft = embed_txt(input_txt, idfs, midf)\n",
        "      ft_index_pharm_val = ft_index_pharm.get_nns_by_vector(vect_ft, 3)\n",
        "      for item in ft_index_pharm_val:\n",
        "          prod, country, doc = index_map_pharm[item]\n",
        "          context.bot.send_message(chat_id=update.message.chat_id, text=\"Производитель: {} Страна производства: {} Документация препарата: {}\".format(prod, country, doc))\n",
        "      return\n",
        "\n",
        "    # QA - болталка\n",
        "    vect_ft = embed_txt(input_txt, {}, 1)\n",
        "    ft_index_val, distances = ft_index.get_nns_by_vector(vect_ft, 1, include_distances=True)\n",
        "\n",
        "    # - уточнение вопроса при большом отклонении входного вектора\n",
        "    if distances[0] > 0.5:\n",
        "      print(distances[0])\n",
        "      context.bot.send_message(chat_id=update.message.chat_id, text=\"Мне Ваш запрос непонятен. Попробуйте перефразировать.\")\n",
        "      return\n",
        "\n",
        "    #ответ бота на свободную тему\n",
        "    context.bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])\n",
        "\n",
        "start_command_handler = CommandHandler('start', startCommand)\n",
        "text_message_handler = MessageHandler(filters.Filters.text, textMessage)\n",
        "dispatcher.add_handler(start_command_handler)\n",
        "dispatcher.add_handler(text_message_handler)\n",
        "updater.start_polling(clean=True)\n",
        "updater.idle()"
      ]
    }
  ]
}